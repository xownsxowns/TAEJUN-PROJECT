{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Convolution2D, Activation, MaxPooling2D, Dense, BatchNormalization, Dropout\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.utils import np_utils\n",
    "from keras.constraints import maxnorm\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = 30000\n",
    "training_inputs = X_train[0:N_train,:,:] / 255.0\n",
    "training_targets = np_utils.to_categorical(y_train)[0:N_train]\n",
    "\n",
    "val_inputs = X_train[(N_train+1):42000,:,:] / 255.0\n",
    "val_targets = np_utils.to_categorical(y_train)[(N_train+1):42000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_inputs = training_inputs.reshape(training_inputs.shape[0], 784)\n",
    "val_inputs = val_inputs.reshape(val_inputs.shape[0], 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer by layer pretraining Models (greedy layer-wise training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ba...)`\n"
     ]
    }
   ],
   "source": [
    "input_img = Input(shape = (784, ))\n",
    "distorted_input1 = Dropout(.1)(input_img)\n",
    "encoded1 = Dense(800, activation = 'sigmoid')(distorted_input1)\n",
    "encoded1_bn = BatchNormalization()(encoded1)\n",
    "decoded1 = Dense(784, activation = 'sigmoid')(encoded1_bn)\n",
    "\n",
    "autoencoder1 = Model(input=input_img, output=decoded1)\n",
    "encoder1 = Model(input=input_img, output=encoded1_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ba...)`\n"
     ]
    }
   ],
   "source": [
    "encoded1_input = Input(shape = (800,))\n",
    "distorted_input2 = Dropout(.2)(encoded1_input)\n",
    "encoded2 = Dense(400, activation='sigmoid')(distorted_input2)\n",
    "encoded2_bn = BatchNormalization()(encoded2)\n",
    "decoded2 = Dense(800, activation='sigmoid')(encoded2_bn)\n",
    "\n",
    "autoencoder2 = Model(input=encoded1_input, output=decoded2)\n",
    "encoder2 = Model(input=encoded1_input, output=encoded2_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:7: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ba...)`\n"
     ]
    }
   ],
   "source": [
    "encoded2_input = Input(shape = (400,))\n",
    "distorted_input3 = Dropout(.3)(encoded2_input)\n",
    "encoded3 = Dense(200, activation='sigmoid')(distorted_input3)\n",
    "encoded3_bn = BatchNormalization()(encoded3)\n",
    "decoded3 = Dense(400, activation='sigmoid')(encoded3_bn)\n",
    "\n",
    "autoencoder3 = Model(input=encoded2_input, output=decoded3)\n",
    "encoder3 = Model(input=encoded2_input, output=encoded3_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deep Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:11: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "encoded1_da = Dense(800, activation='sigmoid')(input_img)\n",
    "encoded1_da_bn = BatchNormalization()(encoded1_da)\n",
    "encoded2_da = Dense(400, activation='sigmoid')(encoded1_da_bn)\n",
    "encoded2_da_bn = BatchNormalization()(encoded2_da)\n",
    "encoded3_da = Dense(200, activation='sigmoid')(encoded2_da_bn)\n",
    "encoded3_da_bn = BatchNormalization()(encoded3_da)\n",
    "decoded3_da = Dense(400, activation='sigmoid')(encoded3_da_bn)\n",
    "decoded2_da = Dense(800, activation='sigmoid')(decoded3_da)\n",
    "decoded1_da = Dense(784, activation='sigmoid')(decoded2_da)\n",
    "\n",
    "deep_autoencoder = Model(input=input_img, output=decoded1_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:8: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n"
     ]
    }
   ],
   "source": [
    "nad_encoded1_da = Dense(800, activation='sigmoid')(input_img)\n",
    "nad_encoded1_da_bn = BatchNormalization()(nad_encoded1_da)\n",
    "nad_encoded2_da = Dense(400, activation='sigmoid')(nad_encoded1_da_bn)\n",
    "nad_encoded2_da_bn = BatchNormalization()(nad_encoded2_da)\n",
    "nad_decoded2_da = Dense(800, activation='sigmoid')(nad_encoded2_da_bn)\n",
    "nad_decoded1_da = Dense(784, activation='sigmoid')(nad_decoded2_da)\n",
    "\n",
    "nad_deep_autoencoder = Model(input=input_img, output=nad_decoded1_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.25\n",
      "3.6125\n",
      "3.0706249999999997\n",
      "2.6100312499999996\n",
      "2.2185265624999997\n",
      "1.8857475781249997\n",
      "1.60288544140625\n",
      "1.3624526251953124\n",
      "1.1580847314160156\n",
      "0.9843720217036133\n",
      "0.8367162184480713\n",
      "0.7112087856808607\n"
     ]
    }
   ],
   "source": [
    "sgd1 = SGD(lr = 5, decay = 0.5, momentum = 0.85, nesterov = True)\n",
    "sgd2 = SGD(lr = 5, decay = 0.5, momentum = 0.85, nesterov = True)\n",
    "sgd3 = SGD(lr = 5, decay = 0.5, momentum = 0.85, nesterov = True)\n",
    "\n",
    "autoencoder1.compile(loss='binary_crossentropy', optimizer = sgd1)\n",
    "autoencoder2.compile(loss='binary_crossentropy', optimizer = sgd2)\n",
    "autoencoder3.compile(loss='binary_crossentropy', optimizer = sgd3)\n",
    "\n",
    "deep_autoencoder.compile(loss='binary_crossentropy', optimizer = sgd1)\n",
    "nad_deep_autoencoder.compile(loss='binary_crossentropy', optimizer=sgd1)\n",
    "\n",
    "# what will happen to the learning rates under this decay schedule?\n",
    "lr = 5\n",
    "for i in range(12):\n",
    "    lr = lr - lr*.15\n",
    "    print(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training first autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 9000 samples\n",
      "Epoch 1/8\n",
      "21000/21000 [==============================] - 3s 166us/step - loss: 0.5242 - val_loss: 0.3446\n",
      "Epoch 2/8\n",
      "21000/21000 [==============================] - 3s 146us/step - loss: 0.3039 - val_loss: 0.2268\n",
      "Epoch 3/8\n",
      "21000/21000 [==============================] - 3s 148us/step - loss: 0.2285 - val_loss: 0.1940\n",
      "Epoch 4/8\n",
      "21000/21000 [==============================] - 3s 146us/step - loss: 0.1989 - val_loss: 0.1789\n",
      "Epoch 5/8\n",
      "21000/21000 [==============================] - 3s 146us/step - loss: 0.1836 - val_loss: 0.1699\n",
      "Epoch 6/8\n",
      "21000/21000 [==============================] - 3s 146us/step - loss: 0.1742 - val_loss: 0.1640\n",
      "Epoch 7/8\n",
      "21000/21000 [==============================] - 3s 146us/step - loss: 0.1678 - val_loss: 0.1594\n",
      "Epoch 8/8\n",
      "21000/21000 [==============================] - 3s 146us/step - loss: 0.1631 - val_loss: 0.1559\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2918bc96588>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder1.fit(training_inputs, training_inputs, nb_epoch=8, batch_size=512, validation_split=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 800)\n"
     ]
    }
   ],
   "source": [
    "first_layer_code = encoder1.predict(training_inputs)\n",
    "print(first_layer_code.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training second autoencoder\n",
    "encoder1에서 나온 first_layer_code를 second autoencoder의 input 및 output으로 넣어서 학습시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 7500 samples\n",
      "Epoch 1/8\n",
      "22500/22500 [==============================] - 2s 99us/step - loss: -1.4441 - val_loss: -3.0353\n",
      "Epoch 2/8\n",
      "22500/22500 [==============================] - 2s 87us/step - loss: -3.8331 - val_loss: -4.4452\n",
      "Epoch 3/8\n",
      "22500/22500 [==============================] - 2s 88us/step - loss: -4.6129 - val_loss: -4.7745\n",
      "Epoch 4/8\n",
      "22500/22500 [==============================] - 2s 87us/step - loss: -4.8228 - val_loss: -4.8998\n",
      "Epoch 5/8\n",
      "22500/22500 [==============================] - 2s 87us/step - loss: -4.9205 - val_loss: -4.9736\n",
      "Epoch 6/8\n",
      "22500/22500 [==============================] - 2s 88us/step - loss: -4.9831 - val_loss: -5.0260\n",
      "Epoch 7/8\n",
      "22500/22500 [==============================] - 2s 85us/step - loss: -5.0281 - val_loss: -5.0662\n",
      "Epoch 8/8\n",
      "22500/22500 [==============================] - 2s 87us/step - loss: -5.0636 - val_loss: -5.0985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x291a1d8f320>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder2.fit(first_layer_code, first_layer_code, nb_epoch=8, batch_size=512, validation_split=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 400)\n"
     ]
    }
   ],
   "source": [
    "second_layer_code = encoder2.predict(first_layer_code)\n",
    "print(second_layer_code.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training third autoencoder\n",
    "encoder2에서 나온 second_layer_code를 third autoencoder의 input 및 output으로 넣어서 학습시킴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:1: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21000 samples, validate on 9000 samples\n",
      "Epoch 1/8\n",
      "21000/21000 [==============================] - 1s 43us/step - loss: -11.2100 - val_loss: -15.7451\n",
      "Epoch 2/8\n",
      "21000/21000 [==============================] - 1s 32us/step - loss: -15.7478 - val_loss: -16.0841\n",
      "Epoch 3/8\n",
      "21000/21000 [==============================] - 1s 32us/step - loss: -15.9443 - val_loss: -16.1994\n",
      "Epoch 4/8\n",
      "21000/21000 [==============================] - 1s 33us/step - loss: -16.0366 - val_loss: -16.2698\n",
      "Epoch 5/8\n",
      "21000/21000 [==============================] - 1s 32us/step - loss: -16.0962 - val_loss: -16.3219\n",
      "Epoch 6/8\n",
      "21000/21000 [==============================] - 1s 32us/step - loss: -16.1402 - val_loss: -16.3612\n",
      "Epoch 7/8\n",
      "21000/21000 [==============================] - 1s 31us/step - loss: -16.1769 - val_loss: -16.3943\n",
      "Epoch 8/8\n",
      "21000/21000 [==============================] - 1s 31us/step - loss: -16.2067 - val_loss: -16.4229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x291a19929e8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoencoder3.fit(second_layer_code, second_layer_code, nb_epoch=8, batch_size=512, validation_split=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the weights of the deep autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_autoencoder.layers[1].set_weights(autoencoder1.layers[2].get_weights()) # first dense layer (800)\n",
    "deep_autoencoder.layers[2].set_weights(autoencoder1.layers[3].get_weights()) # first bn layer\n",
    "deep_autoencoder.layers[3].set_weights(autoencoder2.layers[2].get_weights()) # second dense layer\n",
    "deep_autoencoder.layers[4].set_weights(autoencoder2.layers[3].get_weights()) # second bn layer\n",
    "deep_autoencoder.layers[5].set_weights(autoencoder3.layers[2].get_weights()) # third dense layer\n",
    "deep_autoencoder.layers[6].set_weights(autoencoder3.layers[3].get_weights()) # third bn layer\n",
    "deep_autoencoder.layers[7].set_weights(autoencoder3.layers[4].get_weights()) # first decoder\n",
    "deep_autoencoder.layers[8].set_weights(autoencoder2.layers[4].get_weights()) # second decoder\n",
    "deep_autoencoder.layers[9].set_weights(autoencoder1.layers[4].get_weights()) # third decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the weights of the not-as-deep autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "nad_deep_autoencoder.layers[1].set_weights(autoencoder1.layers[2].get_weights()) # first dense layer\n",
    "nad_deep_autoencoder.layers[2].set_weights(autoencoder1.layers[3].get_weights()) # first bn layer\n",
    "nad_deep_autoencoder.layers[3].set_weights(autoencoder2.layers[2].get_weights()) # second dense layer\n",
    "nad_deep_autoencoder.layers[4].set_weights(autoencoder2.layers[3].get_weights()) # second bn layer\n",
    "nad_deep_autoencoder.layers[5].set_weights(autoencoder2.layers[4].get_weights()) # second decoder\n",
    "nad_deep_autoencoder.layers[6].set_weights(autoencoder1.layers[4].get_weights()) # third decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_inputs = nad_deep_autoencoder.predict(training_inputs[0:25,])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.17241056,  0.12108864,  0.1097716 , ...,  0.1520464 ,\n",
       "         0.19927366,  0.12669806],\n",
       "       [ 0.27290544,  0.13680121,  0.1488803 , ...,  0.15962237,\n",
       "         0.20814535,  0.1475452 ],\n",
       "       [ 0.261457  ,  0.12972115,  0.13423651, ...,  0.20445113,\n",
       "         0.17800608,  0.16758651],\n",
       "       ..., \n",
       "       [ 0.21481082,  0.17583218,  0.13250706, ...,  0.28173161,\n",
       "         0.24388289,  0.23236974],\n",
       "       [ 0.28083202,  0.21616349,  0.21145223, ...,  0.31591725,\n",
       "         0.32515559,  0.16630042],\n",
       "       [ 0.3799876 ,  0.17458279,  0.14970441, ...,  0.23168245,\n",
       "         0.21439865,  0.18295565]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On to 'fine-tuning' for classification \n",
    "전체 모델을 supervised learning으로 fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:5: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "C:\\Users\\jhpark\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:9: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 7500 samples\n",
      "Epoch 1/6\n",
      "22500/22500 [==============================] - 4s 199us/step - loss: 1.0976 - acc: 0.6627 - val_loss: 0.3847 - val_acc: 0.8783\n",
      "Epoch 2/6\n",
      "22500/22500 [==============================] - 4s 183us/step - loss: 0.2916 - acc: 0.9120 - val_loss: 0.2343 - val_acc: 0.9268\n",
      "Epoch 3/6\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1915 - acc: 0.9418 - val_loss: 0.1997 - val_acc: 0.9361\n",
      "Epoch 4/6\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1372 - acc: 0.9591 - val_loss: 0.1694 - val_acc: 0.9476\n",
      "Epoch 5/6\n",
      "22500/22500 [==============================] - 4s 185us/step - loss: 0.1048 - acc: 0.9680 - val_loss: 0.1589 - val_acc: 0.9524\n",
      "Epoch 6/6\n",
      "22500/22500 [==============================] - 4s 186us/step - loss: 0.0820 - acc: 0.9745 - val_loss: 0.1583 - val_acc: 0.9541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x291a3b886d8>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense1 = Dense(500, activation='relu')(nad_decoded1_da)\n",
    "dense1_drop = Dropout(.3)(dense1)\n",
    "dense2 = Dense(10, activation='sigmoid')(dense1_drop)\n",
    "\n",
    "classifier = Model(input=input_img, output=dense2)\n",
    "sgd4 = SGD(lr=.1, decay=0.001, momentum=0.95, nesterov=True)\n",
    "classifier.compile(loss='categorical_crossentropy', optimizer=sgd4, metrics=['accuracy'])\n",
    "\n",
    "classifier.fit(training_inputs, training_targets, nb_epoch=6, batch_size=600, validation_split=0.25, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 800)               628000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 800)               3200      \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 400)               320400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 400)               1600      \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 800)               320800    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 784)               627984    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 10)                5010      \n",
      "=================================================================\n",
      "Total params: 2,299,494\n",
      "Trainable params: 2,297,094\n",
      "Non-trainable params: 2,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = classifier.predict(val_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 9, 6, 4, 4, 1, 7, 0, 9, 3, 5, 8, 2, 7, 9, 7, 7, 8, 5, 7, 9, 6,\n",
       "       6, 6], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.argmax(val_preds, axis=1)\n",
    "true_digits = np.argmax(val_targets, axis=1)\n",
    "predictions[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 9, 6, 4, 4, 1, 7, 0, 9, 3, 5, 8, 2, 7, 4, 7, 7, 8, 5, 7, 9, 6,\n",
       "       6, 6], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_digits[0:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
