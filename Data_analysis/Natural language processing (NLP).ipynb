{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import urllib\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TICKER = 'CSCO' # 주식명 설정\n",
    "URL_TEMPLATE = \"https://feeds.finance.yahoo.com/\" + \\\n",
    "    \"rss/2.0/headline?s=%s%region=US&lang=en-US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Taejun/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') # 불용어를 내려받는다. 불용어란 제외하는 편이 나은 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_urls(ticker):\n",
    "    # 해당 주식과 관련된 기사를 반환한다.\n",
    "    link_pattern = re.compile(r\"<link>[^<]*</link>\")\n",
    "    xml_url = URL_TEMPLATE % ticker\n",
    "    xml_data = urllib.request.urlopen(xml_url).read().decode('utf-8')\n",
    "    link_hits = re.findall(link_pattern, xml_data)\n",
    "    return [h[6:-7] for h in link_hits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_content(url):\n",
    "    '''입력: 신문기사 url\n",
    "    출력: 신문 기사 전처리 결과\n",
    "    HTML 파일을 내려받은 뒤\n",
    "    각 문단의 내용을 정리한다.\n",
    "    ''' \n",
    "    paragraph_re = re.compile(r\"<p>.*</p>\")\n",
    "    tag_re = re.compile(r\"<[^>]*>\")\n",
    "    raw_html = urllib.request.urlopen(url).read().decode('utf-8')\n",
    "    paragraphs = re.findall(paragraph_re, raw_html)\n",
    "    all_text = \"\".join(paragraphs)\n",
    "    content = re.sub(tag_re, \"\", all_text)\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_bag(txt):\n",
    "    '''입력: 문자열\n",
    "    출력: BoW(Bag-of-words) 특징값\n",
    "    불용어(stop words)는 무시하고 처리한다.'''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    txt_as_ascii = txt.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(txt_as_ascii)\n",
    "    words = [t for t in tokens if t.isalpha()]\n",
    "    lemmas = [lemmatizer.lemmatize(w) for w in words]\n",
    "    # lemmatization은 다양한 형태로 등장하는 같은 단어를 처리하는 과정\n",
    "    stop = set(stopwords.words('english'))\n",
    "    nostops = [l for l in lemmas if l not in stop]\n",
    "    return nltk.FreqDist(nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_good_bad(bag):\n",
    "    '''입력: BoW 특징값\n",
    "    출력: 긍정/부정적 단어의 개수'''\n",
    "    # BoW는 자연어 처리에서 널리 쓰이는 가장 기본적인 특징값으로, 문장이나 문서 등 문자열 데이터를 벡터로 표현\n",
    "    good_synsets = set(wn.synsets('good')+wn.synsets('up'))\n",
    "    bad_synsets = set(wn.synsets('bad')+wn.synsets('donw'))\n",
    "    n_good, n_bad = 0,0\n",
    "    for lemma, ct in bag.items():\n",
    "        ss = wn.synsets(lemma)\n",
    "        if good_synsets.intersection(ss): n_good += 1\n",
    "        if bad_synsets.intersection(ss): n_bad += 1\n",
    "    return n_good, n_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not enough arguments for format string",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-12ddfdbd5f29>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0murls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_article_urls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTICKER\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mget_article_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext_to_bag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontents\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcount_good_bad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtxt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbags\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mn_good_articles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcounts\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-dd729bd9ebaa>\u001b[0m in \u001b[0;36mget_article_urls\u001b[0;34m(ticker)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# 해당 주식과 관련된 기사를 반환한다.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlink_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"<link>[^<]*</link>\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mxml_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mURL_TEMPLATE\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mticker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mxml_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxml_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mlink_hits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink_pattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxml_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: not enough arguments for format string"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "urls = get_article_urls(TICKER)\n",
    "contents = [get_article_content(u) for u in urls]\n",
    "bags = [text_to_bag(txt) for txt in contents]\n",
    "counts = [count_good_bad(txt) for txt in bags]\n",
    "n_good_articles = len([_ for g, b in counts if g > b])\n",
    "n_bad_articles = len([_ for g, b in counts if g < b])\n",
    "\n",
    "print(\"긍정적인 기사: %i개, 부정적인 기사: %i개\" %\n",
    "     (n_good_articles, n_bad_articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 품사 태깅\n",
    "nltk.pos_tag([\"I\", \"drink\", \"milk\"])\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "# 두 벡터를 비교할때 많이 씀. 벡터 내적 연산의 정의에 의해 크기가 1인 벡터의 내적은 두 벡터가 이루는 각도의 코사인 값이 됩니다. \n",
    "# 그 결과 동일한 벡터의 코사인 유사도는 1이 나오고 단어가 전혀 겹치지 않으면 0이 나옵니다.\n",
    "# BoW 특징값 벡터는 모든 성분이 양수이기 때문에 0보다 작은 코사인 유사도는 나오지 않습니다."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
